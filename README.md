# ğŸ”¬ Scalar Autograd Engine (From Scratch)

A **minimal automatic differentiation engine for scalar values**, built from the ground up to understand how **backpropagation and computation graphs** actually work.

This project mirrors the core ideas behind **PyTorch Autograd**, but strips everything down to the bare essentials so the mechanics are completely transparent.

---

## âœ¨ Highlights

- ğŸ§  Custom **scalar autograd engine**
- ğŸ”— Dynamic **computation graph construction**
- â• Operator overloading (`+`, `*`, `**`)
- ğŸ”„ Manual backward functions (chain rule)
- ğŸ“ Gradient accumulation
- ğŸ“Š **Graphviz visualization** of the computation graph
- ğŸ“ Designed purely for learning & clarity

---

## ğŸ“Œ Core Concept

Each scalar value is treated as a node in a **directed acyclic graph (DAG)**.

- **Value nodes** store:
  - numerical data
  - gradient
- **Operation nodes** represent computations (`+`, `*`, `**`)
- **Edges** encode dependencies between values

During backpropagation:
- gradients flow **from output â†’ inputs**
- each operation applies the **chain rule**
- gradients accumulate at leaf nodes

---

## ğŸ§± File Structure

Practice/
â”œâ”€â”€ ScalarDerivative.ipynb # Main notebook (engine + examples)
â””â”€â”€ README.md


---

## ğŸš€ Example

```python
a = Value(5, label="A")
b = Value(6, label="B")

c = a * b
d = c + a

d.backward()```


This builds the computation graph dynamically and computes gradients for a and b.

ğŸ“Š **Graph Visualization**

The computation graph can be rendered using Graphviz, where:

ğŸŸ¦ Rectangular nodes â†’ scalar values (data, grad)

âšª Circular nodes â†’ operations (+, *)

â¡ï¸ Directed edges â†’ data flow

This makes gradient propagation explicit and visual, which is extremely useful for understanding backprop.

ğŸ¯ **Why this exists**

This project is meant to help you:

truly understand how autograd works internally

demystify backpropagation

see computation graphs instead of just equations

connect math â†’ code â†’ deep learning frameworks

If you understand this notebook, you understand the core of modern deep learning frameworks.

âš ï¸ Limitations (Intentional)

Scalars only (no tensors)

No broadcasting

No vectorization

No performance optimizations

These constraints keep the implementation simple, readable, and educational.

ğŸ“š Inspiration

PyTorch Autograd

micrograd by Andrej Karpathy

Computational graph theory

ğŸ Final Note

This project is not about speed or scale.

Itâ€™s about understanding.

If you can build autograd for scalars, you truly understand backpropagation.

â­ If this helped you, consider starring the repo!
