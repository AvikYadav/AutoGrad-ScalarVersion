ğŸ§® Scalar Autograd Engine (From Scratch)

This notebook implements a minimal automatic differentiation engine for scalar values, inspired by how frameworks like PyTorch Autograd work internally â€” but built completely from scratch for learning purposes.

The goal is to understand backpropagation, computation graphs, and gradient flow at the most fundamental level.

ğŸ“Œ What this project does

Implements a Value class representing a scalar with gradient tracking

Builds a computation graph dynamically using Python operator overloading

Supports basic operations:

Addition (+)

Multiplication (*)

Power (**)

Manually defines local backward functions for each operation

Accumulates gradients via reverse-mode automatic differentiation

Visualizes the computation graph using Graphviz

This is a learning-focused implementation, not a performance-oriented one.

ğŸ§  Core Idea

Every scalar value is treated as a node in a graph:

Value nodes store:

numerical data

gradient

Operation nodes represent computations (+, *, etc.)

Edges encode dependencies between values

During backpropagation:

Gradients flow from output to inputs

Each node applies the chain rule

Gradients are accumulated at leaf nodes

âœ¨ Features

âœ” Custom scalar autograd engine

âœ” Dynamic computation graph creation

âœ” Operator overloading (__add__, __mul__, __pow__)

âœ” Manual backward functions

âœ” Gradient accumulation

âœ” Clean Graphviz visualization of the graph

âœ” Educational and easy to extend

ğŸ“‚ File Structure
Practice/
â”œâ”€â”€ ScalarDerivative.ipynb   # Main notebook (implementation + examples)
â””â”€â”€ README.md

ğŸš€ Example Usage
a = Value(5, label="A")
b = Value(6, label="B")
c = a * b
d = c + a

d.backward()


This builds a computation graph internally and computes gradients for a and b.

ğŸ“Š Graph Visualization

The computation graph can be rendered using Graphviz, showing:

Rectangular nodes â†’ scalar values (data, grad)

Circular nodes â†’ operations (+, *)

Directed edges â†’ data flow

This makes gradient flow explicit and intuitive.

ğŸ§ª Why this project exists

This project is meant to help you:

Understand how autograd works internally

Demystify backpropagation

See how computation graphs are built and traversed

Bridge the gap between math and deep learning frameworks

If you understand this notebook, you understand the core of PyTorch autograd.

âš ï¸ Limitations (Intentional)

Scalars only (no tensors)

No broadcasting

No vectorization

No performance optimizations

These limitations are intentional to keep the logic transparent and educational.

ğŸ”® Possible Extensions

Add more operations (tanh, relu, exp)

Implement topological sorting explicitly

Add tensor support

Visualize backward pass separately

Compare gradients with PyTorch

ğŸ“š Inspiration

PyTorch Autograd

micrograd by Andrej Karpathy

Computational graph theory

ğŸ Final Note

This notebook is not about efficiency â€” itâ€™s about clarity.

â€œIf you can build autograd for scalars, you truly understand backpropagation.â€